// Supabase Edge Function for fetching LinkedIn metrics
// Triggered by a pg_cron job that runs every hour

import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import { createClient } from "https://esm.sh/@supabase/supabase-js@2";
// Import from bundled shared code (generated by npm run bundle:shared)
import { LinkedInClient } from "../_shared/linkedin/client.ts";
import { getLinkedInToken } from "../_shared/token-utils.ts";

interface CronJobLog {
  id?: number;
  job_name: string;
  started_at: string;
  completed_at?: string;
  status: "running" | "completed" | "failed";
  posts_processed: number;
  successes: number;
  failures: number;
  error_details?: any;
}

interface ProcessedPost {
  linkedin_post_id: string;
  user_id: string;
  organization_id?: string;
  published_at: string;
}

interface MetricsResult {
  processed: number;
  succeeded: number;
  failed: number;
  errors: string[];
}

serve(async (req) => {
  const startTime = new Date().toISOString();
  const jobName = "fetch-linkedin-metrics";
  let logId: number | null = null;

  // Clean logging function
  const log = (
    level: "info" | "warn" | "error",
    message: string,
    data?: any
  ) => {
    if (level === "error") {
      console.error(message);
    } else if (level === "warn") {
      console.warn(message);
    } else {
      console.log(message);
    }
  };

  try {
    log("info", "Starting LinkedIn metrics fetch job");

    // Initialize Supabase client
    const supabaseUrl = Deno.env.get("SUPABASE_URL")!;
    const supabaseServiceKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!;

    const supabase = createClient(supabaseUrl, supabaseServiceKey, {
      auth: {
        autoRefreshToken: false,
        persistSession: false,
      },
    });

    // Create initial log entry
    const { data: logData, error: logError } = await supabase
      .from("cron_job_logs")
      .insert({
        job_name: jobName,
        started_at: startTime,
        status: "running",
        posts_processed: 0,
        successes: 0,
        failures: 0,
      })
      .select()
      .single();

    if (logError) {
      log("error", "Failed to create log entry");
    } else {
      logId = logData.id;
    }

    // Get all published posts (matching existing query structure)
    const { data: posts, error: postsError } = await supabase
      .from("linkedin_posts")
      .select(
        `
        linkedin_post_id,
        user_id,
        organization_id,
        published_at
      `
      )
      .eq("status", "PUBLISHED")
      .not("linkedin_post_id", "is", null);

    if (postsError) {
      log("error", "Failed to fetch posts");
      throw new Error(`Failed to fetch posts: ${postsError.message}`);
    }

    if (!posts || posts.length === 0) {
      log("info", "No published posts found");
      await updateLog(supabase, logId, {
        status: "completed",
        completed_at: new Date().toISOString(),
        posts_processed: 0,
        successes: 0,
        failures: 0,
      });

      return new Response(
        JSON.stringify({
          processed: 0,
          succeeded: 0,
          failed: 0,
          errors: [],
          message: "No published posts found",
        }),
        {
          headers: { "Content-Type": "application/json" },
          status: 200,
        }
      );
    }

    log("info", `Processing ${posts.length} published posts`);

    // Process posts with age-based filtering
    const result = await processPosts(supabase, posts, log);

    // Log final results
    if (result.failed > 0) {
      log(
        "warn",
        `Job completed: ${result.succeeded} succeeded, ${result.failed} failed`
      );
    } else {
      log("info", `Job completed: ${result.succeeded} succeeded`);
    }

    // Update log with final results
    await updateLog(supabase, logId, {
      status: result.failed > 0 ? "failed" : "completed",
      completed_at: new Date().toISOString(),
      posts_processed: result.processed,
      successes: result.succeeded,
      failures: result.failed,
      error_details: result.errors.length > 0 ? result.errors : null,
    });

    return new Response(JSON.stringify(result), {
      headers: { "Content-Type": "application/json" },
      status: 200,
    });
  } catch (error) {
    log("error", "Edge function error");

    // Update log with error
    if (logId) {
      try {
        const supabaseUrl = Deno.env.get("SUPABASE_URL")!;
        const supabaseServiceKey = Deno.env.get("SUPABASE_SERVICE_ROLE_KEY")!;
        const supabase = createClient(supabaseUrl, supabaseServiceKey);

        await updateLog(supabase, logId, {
          status: "failed",
          completed_at: new Date().toISOString(),
          error_details: error.message,
        });
      } catch (logUpdateError) {
        log("error", "Failed to update log with error");
      }
    }

    return new Response(
      JSON.stringify({
        error: error.message,
        processed: 0,
        succeeded: 0,
        failed: 0,
        errors: [error.message],
      }),
      {
        headers: { "Content-Type": "application/json" },
        status: 500,
      }
    );
  }
});

async function processPosts(
  supabase: any,
  posts: ProcessedPost[],
  log: (level: "info" | "warn" | "error", message: string) => void
): Promise<MetricsResult> {
  const result: MetricsResult = {
    processed: 0,
    succeeded: 0,
    failed: 0,
    errors: [],
  };

  const now = new Date();
  const sixtyDaysAgo = new Date(now.getTime() - 60 * 24 * 60 * 60 * 1000);

  // Update metrics for each post (matching updateMetricsForRecentPosts logic)
  const updatePromises = posts.map(async (post) => {
    if (!post.linkedin_post_id) {
      return;
    }

    try {
      result.processed++;

      // Age-based filtering
      const publishedAt = new Date(post.published_at);
      const isOldPost = publishedAt < sixtyDaysAgo;

      if (isOldPost) {
        // For posts older than 60 days, check if metrics already exist for today
        const todayMidnightUTC = new Date();
        todayMidnightUTC.setUTCHours(0, 0, 0, 0);
        const startOfDay = todayMidnightUTC.toISOString();
        const endOfDay = new Date(todayMidnightUTC);
        endOfDay.setUTCDate(endOfDay.getUTCDate() + 1);
        const endOfDayStr = endOfDay.toISOString();

        const { data: existingMetrics } = await supabase
          .from("linkedin_post_metrics")
          .select("id")
          .eq("linkedin_post_id", post.linkedin_post_id)
          .eq("user_id", post.user_id)
          .gte("fetched_at", startOfDay)
          .lt("fetched_at", endOfDayStr)
          .maybeSingle();

        if (existingMetrics) {
          return;
        }
      }

      // Get community token for this user (matching existing logic)
      const communityToken = await getLinkedInToken(post.user_id, "community");
      if (!communityToken?.access_token) {
        log(
          "error",
          `[FETCH_METRICS] Failed to fetch metrics for post: ${post.linkedin_post_id} - No token`
        );
        result.failed++;
        result.errors.push(
          `No community token available for user ${post.user_id}`
        );
        return;
      }

      // Fetch and store metrics (matching existing fetchAndStoreMetrics logic)
      const linkedinAPI = new LinkedInClient(communityToken.access_token);

      // Try organization metrics first if organization_id exists, fallback to personal
      let metrics;
      try {
        metrics = await linkedinAPI.getPostMetrics(
          post.linkedin_post_id,
          post.organization_id
        );
      } catch (orgError) {
        // If organization metrics fail with 400 error, try as personal post
        if (
          orgError.message.includes("400") &&
          orgError.message.includes("Unable to get activityIds")
        ) {
          try {
            metrics = await linkedinAPI.getPostMetrics(post.linkedin_post_id);
          } catch (personalError) {
            // If both fail, throw the original organization error
            throw orgError;
          }
        } else {
          throw orgError;
        }
      }

      await storeMetrics(supabase, post, metrics);

      result.succeeded++;
      log(
        "info",
        `[FETCH_METRICS] Successfully fetched metrics for post: ${post.linkedin_post_id}`
      );
    } catch (error) {
      let errorMessage = "API error";

      if (error.message.includes("token")) {
        errorMessage = "Token expired";
      } else if (error.message.includes("400")) {
        errorMessage = "Bad request";
      } else if (error.message.includes("401")) {
        errorMessage = "Unauthorized";
      } else if (error.message.includes("403")) {
        errorMessage = "Forbidden";
      } else if (error.message.includes("404")) {
        errorMessage = "Post not found";
      } else if (error.message.includes("429")) {
        errorMessage = "Rate limited";
      }

      log(
        "error",
        `[FETCH_METRICS] Failed to fetch metrics for post: ${post.linkedin_post_id} - ${errorMessage}`
      );
      result.failed++;
      result.errors.push(
        `Post ${post.linkedin_post_id} (User: ${post.user_id}): ${error.message}`
      );
    }
  });

  await Promise.all(updatePromises);

  // Log summary
  if (result.failed > 0) {
    log(
      "warn",
      `Processing complete: ${result.succeeded} succeeded, ${result.failed} failed`
    );
  } else {
    log("info", `Processing complete: ${result.succeeded} succeeded`);
  }

  return result;
}

async function storeMetrics(
  supabase: any,
  post: ProcessedPost,
  metrics: any
): Promise<void> {
  // Calculate today's date at midnight UTC for consistent daily records
  const todayMidnightUTC = new Date();
  todayMidnightUTC.setUTCHours(0, 0, 0, 0);
  const fetchedAt = todayMidnightUTC.toISOString();

  // Check if a record already exists for this post and user for today
  // Use a date range to find records for the same day (more robust than exact timestamp match)
  const startOfDay = new Date(fetchedAt);
  const endOfDay = new Date(fetchedAt);
  endOfDay.setUTCDate(endOfDay.getUTCDate() + 1);

  const { data: existingRecord } = await supabase
    .from("linkedin_post_metrics")
    .select("*")
    .eq("linkedin_post_id", post.linkedin_post_id)
    .eq("user_id", post.user_id)
    .gte("fetched_at", startOfDay.toISOString())
    .lt("fetched_at", endOfDay.toISOString())
    .maybeSingle();

  let data, error;

  if (existingRecord) {
    // Update existing record
    const updateResult = await supabase
      .from("linkedin_post_metrics")
      .update({
        impressions: metrics.impressions,
        likes: metrics.likes,
        comments: metrics.comments,
        shares: metrics.shares,
        clicks: metrics.clicks,
        engagement_rate: metrics.engagementRate,
      })
      .eq("id", existingRecord.id)
      .select()
      .single();

    data = updateResult.data;
    error = updateResult.error;
  } else {
    // Insert new record
    const insertResult = await supabase
      .from("linkedin_post_metrics")
      .insert({
        linkedin_post_id: post.linkedin_post_id,
        user_id: post.user_id,
        impressions: metrics.impressions,
        likes: metrics.likes,
        comments: metrics.comments,
        shares: metrics.shares,
        clicks: metrics.clicks,
        engagement_rate: metrics.engagementRate,
        fetched_at: fetchedAt,
      })
      .select()
      .single();

    data = insertResult.data;
    error = insertResult.error;
  }

  if (error) {
    console.error("Error storing LinkedIn post metrics:", error);
    throw new Error(`Failed to store metrics: ${error.message}`);
  }
}

async function updateLog(
  supabase: any,
  logId: number | null,
  updates: Partial<CronJobLog>
): Promise<void> {
  if (!logId) return;

  const { error } = await supabase
    .from("cron_job_logs")
    .update(updates)
    .eq("id", logId);

  if (error) {
    console.error("Failed to update log:", error);
  }
}
